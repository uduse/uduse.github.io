
@online{_ActingRewards_2019,
  title        = {Acting without {{Rewards}}},
  date         = {2019-08-21T16:20:48+00:00},
  url          = {https://ogma.ai/2019/08/acting-without-rewards/},
  urldate      = {2021-09-07},
  abstract     = {Hello, While we continue to work on improving our reinforcement learning (RL) (on two new demos!), here is some information on what else we have tried aside from reinforcement learning for performing tasks with agency. For now regular old RL performs better than what I am about to describe, but perhaps at some point this…},
  langid       = {american},
  organization = {{Ogma Intelligent Systems Corp}},
  file         = {/Users/Uduse/Zotero/storage/FNFDFPMW/acting-without-rewards.html}
}

@online{_ProjectOverviewImproving_,
  title        = {Project {{Overview}} ‹ {{Improving RNN Sequence Generation}} with {{RL}}},
  url          = {https://www.media.mit.edu/projects/improving-rnn-sequence-generation-with-rl/overview/},
  urldate      = {2021-09-07},
  abstract     = {This project investigates a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN) using deep rein...},
  organization = {{MIT Media Lab}},
  file         = {/Users/Uduse/Zotero/storage/IMZHRV5J/overview.html}
}

@online{chen_DecisionTransformerReinforcement_2021,
  title         = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle    = {Decision {{Transformer}}},
  author        = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  date          = {2021-06-24},
  eprint        = {2106.01345},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/2106.01345},
  urldate       = {2021-06-25},
  abstract      = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arXiv},
  file          = {/Users/Uduse/Zotero/storage/EZ6LNB3T/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf;/Users/Uduse/Zotero/storage/ZWMRJ8TL/2106.html}
}

@online{gulcehre_RLUnpluggedSuite_2021,
  title         = {{{RL Unplugged}}: {{A Suite}} of {{Benchmarks}} for {{Offline Reinforcement Learning}}},
  shorttitle    = {{{RL Unplugged}}},
  author        = {Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Tom Le and Colmenarejo, Sergio Gomez and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh and Mankowitz, Daniel and Paduraru, Cosmin and Dulac-Arnold, Gabriel and Li, Jerry and Norouzi, Mohammad and Hoffman, Matt and Nachum, Ofir and Tucker, George and Heess, Nicolas and de Freitas, Nando},
  options       = {useprefix=true},
  date          = {2021-02-12},
  eprint        = {2006.13888},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/2006.13888},
  urldate       = {2021-07-08},
  abstract      = {Offline methods for reinforcement learning have a potential to help bridge the gap between reinforcement learning research and real-world applications. They make it possible to learn policies from offline datasets, thus overcoming concerns associated with online data collection in the real-world, including cost, safety, or ethical concerns. In this paper, we propose a benchmark called RL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes data from a diverse range of domains including games (e.g., Atari benchmark) and simulated motor control problems (e.g., DM Control Suite). The datasets include domains that are partially or fully observable, use continuous or discrete actions, and have stochastic vs. deterministic dynamics. We propose detailed evaluation protocols for each domain in RL Unplugged and provide an extensive analysis of supervised learning and offline RL methods using these protocols. We will release data for all our tasks and open-source all algorithms presented in this paper. We hope that our suite of benchmarks will increase the reproducibility of experiments and make it possible to study challenging tasks with a limited computational budget, thus making RL research both more systematic and more accessible across the community. Moving forward, we view RL Unplugged as a living benchmark suite that will evolve and grow with datasets contributed by the research community and ourselves. Our project page is available on https://git.io/JJUhd.},
  archiveprefix = {arXiv},
  file          = {/Users/Uduse/Zotero/storage/U8LN39JG/Gulcehre et al. - 2021 - RL Unplugged A Suite of Benchmarks for Offline Re.pdf;/Users/Uduse/Zotero/storage/DP76W7LA/2006.html}
}

@online{janner_ReinforcementLearningOne_2021,
  title         = {Reinforcement {{Learning}} as {{One Big Sequence Modeling Problem}}},
  author        = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date          = {2021-06-03},
  eprint        = {2106.02039},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/2106.02039},
  urldate       = {2021-06-29},
  abstract      = {Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as "one big sequence modeling" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.},
  archiveprefix = {arXiv},
  keywords      = {RL,transformer},
  file          = {/Users/Uduse/Zotero/storage/3B5BLAQ9/Janner et al. - 2021 - Reinforcement Learning as One Big Sequence Modelin.pdf;/Users/Uduse/Zotero/storage/G49PWIYJ/2106.html}
}

@online{katharopoulos_TransformersAreRNNs_2020,
  title         = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle    = {Transformers Are {{RNNs}}},
  author        = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
  date          = {2020-08-31},
  eprint        = {2006.16236},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/2006.16236},
  urldate       = {2021-06-25},
  abstract      = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \$\textbackslash mathcal\{O\}\textbackslash left(N\^2\textbackslash right)\$ to \$\textbackslash mathcal\{O\}\textbackslash left(N\textbackslash right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  keywords      = {transformer},
  file          = {/Users/Uduse/Zotero/storage/FRUNDSL3/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;/Users/Uduse/Zotero/storage/TTYNHMQ2/2006.html}
}

@online{schmidhuber_ReinforcementLearningUpside_2020,
  title         = {Reinforcement {{Learning Upside Down}}: {{Don}}'t {{Predict Rewards}} -- {{Just Map Them}} to {{Actions}}},
  shorttitle    = {Reinforcement {{Learning Upside Down}}},
  author        = {Schmidhuber, Juergen},
  date          = {2020-06-23},
  eprint        = {1912.02875},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/1912.02875},
  urldate       = {2021-06-26},
  abstract      = {We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [63] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also also conceptually simplify an approach [60] for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.},
  archiveprefix = {arXiv},
  keywords      = {reward_free},
  file          = {/Users/Uduse/Zotero/storage/Z8V8UJ6S/Schmidhuber - 2020 - Reinforcement Learning Upside Down Don't Predict .pdf;/Users/Uduse/Zotero/storage/GCKXWVMY/1912.html}
}

@online{vaswani_AttentionAllYou_2017,
  title         = {Attention {{Is All You Need}}},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date          = {2017-12-05},
  eprint        = {1706.03762},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/1706.03762},
  urldate       = {2021-06-25},
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords      = {DL,NLP,transformer},
  file          = {/Users/Uduse/Zotero/storage/26362T75/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/Uduse/Zotero/storage/YWPLGGDN/1706.html}
}


