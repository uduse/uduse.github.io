<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Palatino:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script><meta name="description" content="This is a sketch of my recent reflections and future research plan. I am only interested in solving real world complex problems and my ultimate goal is to build a system with general intelligence some"><meta property="og:type" content="article"><meta property="og:title" content="My Reinforcement Learning Research Plan for General Intelligence"><meta property="og:url" content="http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/index.html"><meta property="og:site_name" content="Zeyi Wang"><meta property="og:description" content="This is a sketch of my recent reflections and future research plan. I am only interested in solving real world complex problems and my ultimate goal is to build a system with general intelligence some"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/erfan.png"><meta property="article:published_time" content="2022-06-02T22:57:05.000Z"><meta property="article:modified_time" content="2022-06-04T20:26:28.817Z"><meta property="article:author" content="Zeyi Wang"><meta property="article:tag" content="Artificial General Intelligence (AGI)"><meta property="article:tag" content="Reinforcement Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/erfan.png"><link rel="canonical" href="http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/","path":"2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/","title":"My Reinforcement Learning Research Plan for General Intelligence"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>My Reinforcement Learning Research Plan for General Intelligence | Zeyi Wang</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">Zeyi Wang</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">My Nature Reserve</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li></ul></nav></div><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#what-i-want-to-change"><span class="nav-number">1.</span> <span class="nav-text">What I want to change:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#what-we-should-do"><span class="nav-number">2.</span> <span class="nav-text">What we should do:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#what-i-will-do"><span class="nav-number">3.</span> <span class="nav-text">What I will do:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#closing-thoughts."><span class="nav-number">4.</span> <span class="nav-text">Closing thoughts.</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Zeyi Wang</p><div class="site-description" itemprop="description"></div></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/uduse" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;uduse" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://twitter.com/wzy950618" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;wzy950618" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a></span></div></div></div></div></aside><div class="sidebar-dimmer"></div></header><a href="https://github.com/uduse" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en"><link itemprop="mainEntityOfPage" href="http://example.com/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Zeyi Wang"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Zeyi Wang"><meta itemprop="description" content=""></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="My Reinforcement Learning Research Plan for General Intelligence | Zeyi Wang"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">My Reinforcement Learning Research Plan for General Intelligence</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2022-06-02 16:57:05" itemprop="dateCreated datePublished" datetime="2022-06-02T16:57:05-06:00">2022-06-02</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2022-06-04 14:26:28" itemprop="dateModified" datetime="2022-06-04T14:26:28-06:00">2022-06-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/" itemprop="commentCount"></span></a></span></div></div></header><div class="post-body" itemprop="articleBody"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>This is a sketch of my recent reflections and future research plan. I am only interested in solving real world complex problems and my ultimate goal is to build a system with general intelligence someday. As a result, the discussion here is only for the path to general intelligence.</p><span id="more"></span><p>I also believe that having a solid understanding of intelligence itself is neccessary for building general intelligent systems. Human being is the most successful example we know and I will heavily draw insights from human behavior and intelligence.</p><p><strong>Disclaimer: I am dumb.</strong></p><h2 id="what-i-want-to-change">What I want to change:</h2><p>I think the current problem formulations of RL, especially Markov Decision Process (MDP), deviate too much from the problems I am interested in solving. In real world, the state is always only partially observable to the agent. In fact, the agent's observation is so partial compared to the state that we should completely give up storing and processing the full state. The action space is usually continuous and in multiple dimensions. The reward signals do not really exist. The system dynamics is never known. Real world always moves forward and never resets to a starting state, at least for the real world we could perceive. The real world refreshes at a much finer granularity.</p><p>When we need to solve a real world problem with this formulation, we have to put in great effort to squeeze the real problem into the formulation. Information lost is unavoidable during this process of abstraction, but could be less harmful if we carefully pick the right information to retain. The most significant change is the addition of reward signals. We do this by first imagining a desired behavior for the agent, such as walking or jumping, then craft the reward function to describe such behavior. This extreme compression from a continuum of behavior to a single value is hardly desirable, and sometimes not even possible. For example, how do you craft a reward function for a parkour move? In a way, we have manually performed inverse reinforcement learning (creating a reward function based on some desired behavior) in the process of just defining the problem. I don't think this step is neccessary at all.</p><div style="width:40%;margin:10px auto;padding:10px 10px;min-width:300px"><figure><img src="/2022/06/02/My-Reinforcement-Learning-Research-Plan-for-General-Intelligence/erfan.png" alt="Erfan Miahi backflipping"><figcaption aria-hidden="true"><a target="_blank" rel="noopener" href="https://erfanmhi.github.io/">Erfan Miahi</a> backflipping</figcaption></figure></div><h2 id="what-we-should-do">What we should do:</h2><p><strong>Give up MDP.</strong> There is more differences than similarities between the real world problems and MDP. There are some work on extending MDP, such as adding partial observability, but why not start a new one from scratch? The reason is right now we do not have a better choice.</p><p><strong>Agent should learn more about itself.</strong> The value function we use right now could be interpreted as a compressed description of the behavior of itself. For any complex task this single value is no way near sufficient for the agent to learn its own behavior. We need to compress the behavior of the agent into something with a larger bandwidth. This way, more information regarding the agent's behavior will be retained and it will be easier for the agent to learn from itself. For example, in addition to a value function, AlphaGo also learns a action distribution function, and this is a much richer representation of the agent's behaivor.</p><p><strong>Give up learning from environment rewards.</strong> I do not believe in the reward hypothesis. I think being smart about defining the rewards for a complex task is creating a bad problem to solve. Solving a bad problem is for sure not the best way to get a good solution to the real problem. If there is no reward, what the agent should be learning from then? My answer is that the agent should learn just for the sake of learning.</p><p><strong>Only use continuing tasks, or treat episodic tasks as a special case.</strong> An "episode" of something is an extremely high level concept and we should not inject it when we formulate the problem. Instead, we should think about why we conceptually divide events into discrete windows. This also encourages us to think about solving problems in a more continuous and online fashion.</p><h2 id="what-i-will-do">What I will do:</h2><p>(In-progress) I will try to formulate the problem and design the interface differently. This reduces the time cost and information lost for converting real world problems. For example, episodic tasks are now special cases of continuous tasks by using two additional booleans that signals the episode start/end. This interface will be mostly compatible with our current problem formulations and interfaces but I do think such an update will be beneficial in the long run. We have different programming languages customized for solving different problems and thinking differently. We should have this for RL too.</p><p>I will show that for simple tasks like cart-pole and mountain-car, it is possible to obtain an good policy without using the reward signal to guide its learning. The agent would also learn a coherent map of behaviors that demonstrates different "skills". Additionally, users can interact with the agent's behavior like interacting with large language models.</p><p>I will show that temporal difference (TD) in learning could be used differently. The agent would have two seemingly competing components that work together: one minimizes TD error for the model and learns to predict the TD error, one maximizes predicted TD error by planning with the model. This TD error should be ideally computed by aggregating various auxiliary tasks instead just the rewards.</p><p>I will show that the "goal" is a summarized expectation of the future. When we force the agent to aggressively compress coherent behaviors into discretized embedding, "goals" will appear.</p><p>I will show that a central sequence model could be used to connect all things aforementioned. We will also stack various smaller procedures or networks on top of the sequence model, but all of them will work together to create a coherent behavior of the agent.</p><p>I will also journal my thoughts as much as possible. I have a vague big picture in mind and it will take great effort already just to put it down nicely. If you would like me to expand any topic here, please let me know and I will do my best to either elaborate or show it with a paper.</p><h2 id="closing-thoughts.">Closing thoughts.</h2><p>My deepest fear is that I am actually living in an ivory tower and not realizing it. I have little research output yet I want to make great changes. That is a bad sign. On the one hand, I desperately need some proof for myself so that I have a little more faith in myself. On the other hand, breaking changes always have to go through tons of failures. Maybe I will only have such confidence decades later. Maybe this is what life all about.</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Artificial-General-Intelligence-AGI/" rel="tag"># Artificial General Intelligence (AGI)</a> <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a></div></footer></article></div><div class="comments" id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Zeyi Wang</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/tex-mml-chtml.js","integrity":"sha256-hlC2uSQYTmPsrzGZTEQEg9PZ1a/+SV6VBCTclohf2og="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"uduse","count":true,"i18n":{"disqus":"disqus"}}</script><script src="/js/third-party/comments/disqus.js"></script></body></html>